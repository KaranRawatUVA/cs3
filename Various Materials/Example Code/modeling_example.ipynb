{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLFPKVblsDk8eSxy1MxKms"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling Examples"
      ],
      "metadata": {
        "id": "ra38wBugX12N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the examples below we are using a dataframe (holds our data) which includes the amount of unifying and polarizing words. How this is done can change based on how you do your sentiment analysis.\n",
        "\n",
        "**Main point** is that we are using the amount of unifying and polarizing words (gathered through the sentiment analysis) to determine the political party of the president giving the speech."
      ],
      "metadata": {
        "id": "Vb4gYPTVZF2g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Biserial Correlation"
      ],
      "metadata": {
        "id": "clPaXnGe47tR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "f0LGQTKU4_rJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYseuOIx4lu6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import pointbiserialr\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prep Data"
      ],
      "metadata": {
        "id": "ZMwgjoe3VNbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Political Party as a binary variable for point biserial correlation analysis\n",
        "# df represents dataframe aka where you hold your data\n",
        "df2['Political Party'] = df2['Political Party'].map({'Republican': 0, 'Democrat': 1})\n"
      ],
      "metadata": {
        "id": "xAOnaZS34unn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Point Biserial Correlation Analysis"
      ],
      "metadata": {
        "id": "aEaxDfMxWYgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation, p_value = pointbiserialr(df2['Political Party'], df2['Overall Ratio'])\n",
        "# runs a point biserial correlation analysis\n",
        "print(\"Correlation:\", round(correlation, 5))\n",
        "print(\"p-value:\", round(p_value, 5))\n",
        "\n"
      ],
      "metadata": {
        "id": "UqK2RChP45RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "lebEPnLGWsgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature and target selection\n",
        "# X are the columns that are independent variables\n",
        "# y what we are what we are trying to determine based on the x\n",
        "# (dependent variables)\n",
        "X = df2.drop(columns=[\"Political Party\", \"Name\", \"Overall Language\"]) # X = df2[[\"Political Party\"]]\n",
        "y = df2[\"Political Party\"]\n",
        "\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "# Training set is the data we train the model one (train it what to look for)\n",
        "# the test set is the data we test to see how our model is at predicting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)  # Fit on train data and transform\n",
        "X_test = scaler.transform(X_test)  # Only transform test data\n",
        "\n",
        "# Create and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "# Evaluate the model\n",
        "# print(\"Training Accuracy:\", model.score(X_train, y_train))\n",
        "print(\"Testing Accuracy:\", accuracy_score(y_test, predictions))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(\"                 Predicted Negative  Predicted Positive\")\n",
        "print(\"Actual Negative      TN = {:<5}        FP = {:<5}\".format(cm[0, 0], cm[0, 1]))\n",
        "print(\"Actual Positive      FN = {:<5}        TP = {:<5}\".format(cm[1, 0], cm[1, 1]))\n",
        "# print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
      ],
      "metadata": {
        "id": "qF_ZgxJbWuwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias And Uncertainty"
      ],
      "metadata": {
        "id": "9kDyzhvmW6ZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calibration Curve"
      ],
      "metadata": {
        "id": "3Ns4E-GhYAgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calibration Curve (Reliability Curve)\n",
        "# Measures how well predicted probabilities match the true probabilities\n",
        "\n",
        "from sklearn.calibration import calibration_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get predicted probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
        "\n",
        "# Plot calibration curve\n",
        "plt.plot(prob_pred, prob_true, marker='o', label=\"Calibration Curve\")\n",
        "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Perfect Calibration\")\n",
        "plt.xlabel(\"Predicted Probability\")\n",
        "plt.ylabel(\"Observed Frequency\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xZDA9dQlW_ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brier Score"
      ],
      "metadata": {
        "id": "ytCLEUfVYDrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Brier Score\n",
        "# Measures accuracy of predicted probabilities\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "brier_score = brier_score_loss(y_test, y_prob) # Lower values = lower bias\n",
        "print(f\"Brier Score: {brier_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "mpZX10NIXsyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Uncertainty (Entropy)"
      ],
      "metadata": {
        "id": "shVcqsEwYLam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entropy\n",
        "# Measures uncertainty of predicted probabilities\n",
        "\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Convert probabilities to entropy\n",
        "entropies = entropy([y_prob, 1 - y_prob], axis=0)\n",
        "print(f\"Entropy (Mean): {np.mean(entropies):.4f}\") # Higher values = higher uncertainty\n"
      ],
      "metadata": {
        "id": "UrzDC4XgYGBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Feature Importance"
      ],
      "metadata": {
        "id": "DDXSkWmdYa1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature coefficients\n",
        "coefficients = model.coef_[0]\n",
        "features = X.columns  # Fix: Use X instead of X_train to get feature names\n",
        "\n",
        "# Create a DataFrame for better readability\n",
        "feature_importance = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})\n",
        "# feature_importance['Absolute Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
        "# feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "print(\"Feature Importance for Logistic Regression:\")\n",
        "print(feature_importance)\n"
      ],
      "metadata": {
        "id": "ZrsErqbRYc3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}